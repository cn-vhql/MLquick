import streamlit as st
import pandas as pd
import os
import io
import matplotlib.pyplot as plt
import zipfile
import shutil
from datetime import datetime
import base64
import tempfile
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import numpy as np

# æ–‡æœ¬å¤„ç†ç›¸å…³å¯¼å…¥
import re
import jieba
import jieba.analyse
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import warnings

# å°è¯•å¯¼å…¥nltk
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    st.warning("âš ï¸ NLTKæœªå®‰è£…ï¼Œè‹±æ–‡æ–‡æœ¬å¤„ç†åŠŸèƒ½å—é™")

# æŠ‘åˆ¶jiebaçš„æ—¥å¿—è¾“å‡º
jieba.setLogLevel(jieba.logging.INFO)


def generate_model_id():
    """ç”ŸæˆåŸºäºæ—¥æœŸæ—¶é—´çš„æ¨¡å‹ID"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡"""
    if pd.isna(text) or text == "":
        return "unknown"

    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', str(text)))
    total_chars = len(re.sub(r'\s+', '', str(text)))

    if total_chars == 0:
        return "unknown"

    chinese_ratio = chinese_chars / total_chars
    return "chinese" if chinese_ratio > 0.3 else "english"


def preprocess_text_column(series, language="auto", remove_stopwords=True, min_word_length=2):
    """
    é¢„å¤„ç†æ–‡æœ¬åˆ—
    å‚æ•°:
    - series: pandas Seriesï¼ŒåŒ…å«æ–‡æœ¬æ•°æ®
    - language: "auto", "chinese", "english"
    - remove_stopwords: æ˜¯å¦ç§»é™¤åœç”¨è¯
    - min_word_length: æœ€å°è¯é•¿åº¦
    """
    processed_texts = []

    for text in series:
        if pd.isna(text) or text == "":
            processed_texts.append("")
            continue

        text = str(text).strip()

        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        if language == "auto":
            detected_lang = detect_language(text)
        else:
            detected_lang = language

        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)  # ä¿ç•™ä¸­è‹±æ–‡å’Œæ•°å­—
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼

        if detected_lang == "chinese":
            # ä¸­æ–‡åˆ†è¯å¤„ç†
            words = jieba.lcut(text)

            # ç§»é™¤åœç”¨è¯ï¼ˆåŸºç¡€ä¸­æ–‡åœç”¨è¯ï¼‰
            if remove_stopwords:
                chinese_stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
                words = [word for word in words if word not in chinese_stopwords and len(word) >= min_word_length]

            processed_text = ' '.join(words)

        else:
            # è‹±æ–‡å¤„ç†
            text = text.lower()
            words = text.split()

            # ç§»é™¤åœç”¨è¯ï¼ˆä½¿ç”¨nltkï¼‰
            if remove_stopwords and NLTK_AVAILABLE:
                try:
                    stop_words = set(stopwords.words('english'))
                    words = [word for word in words if word not in stop_words and len(word) >= min_word_length]
                except:
                    # å¦‚æœnltkæ•°æ®æœªä¸‹è½½ï¼Œä½¿ç”¨åŸºç¡€åœç”¨è¯
                    basic_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}
                    words = [word for word in words if word not in basic_stopwords and len(word) >= min_word_length]

            processed_text = ' '.join(words)

        processed_texts.append(processed_text)

    return pd.Series(processed_texts)


def extract_text_features(text_data, max_features=1000, method="tfidf"):
    """
    ä»æ–‡æœ¬æ•°æ®æå–ç‰¹å¾
    å‚æ•°:
    - text_data: é¢„å¤„ç†åçš„æ–‡æœ¬æ•°æ®ï¼ˆSeriesï¼‰
    - max_features: æœ€å¤§ç‰¹å¾æ•°
    - method: "tfidf" æˆ– "count"
    """
    if method == "tfidf":
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),  # 1-gramå’Œ2-gram
            min_df=2,  # è‡³å°‘å‡ºç°åœ¨2ä¸ªæ–‡æ¡£ä¸­
            max_df=0.8  # æœ€å¤šå‡ºç°åœ¨80%çš„æ–‡æ¡£ä¸­
        )
    else:
        vectorizer = CountVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

    try:
        features = vectorizer.fit_transform(text_data)
        feature_names = vectorizer.get_feature_names_out()
        return features, feature_names, vectorizer
    except Exception as e:
        st.error(f"æ–‡æœ¬ç‰¹å¾æå–å¤±è´¥: {str(e)}")
        return None, None, None


def create_text_visualizations(text_data, labels=None, title="æ–‡æœ¬åˆ†æ"):
    """åˆ›å»ºæ–‡æœ¬åˆ†æå¯è§†åŒ–"""
    visualizations = {}

    try:
        # ç”Ÿæˆè¯äº‘å›¾
        all_text = ' '.join(text_data.dropna().astype(str))
        if all_text.strip():
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                font_path=None,  # ä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½éœ€è¦æŒ‡å®šå­—ä½“è·¯å¾„
                colormap='viridis'
            ).generate(all_text)

            fig = plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'{title} - è¯äº‘å›¾')

            # è½¬æ¢ä¸ºplotlyå›¾è¡¨
            img_buf = io.BytesIO()
            plt.savefig(img_buf, format='png', bbox_inches='tight')
            img_buf.seek(0)
            img_data = img_buf.getvalue()

            # ä½¿ç”¨plotlyæ˜¾ç¤ºå›¾ç‰‡
            fig_plotly = px.imshow(
                plt.imread(img_buf),
                title=f'{title} - è¯äº‘å›¾'
            )
            visualizations['wordcloud'] = fig_plotly
            plt.close()

        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ›å»ºä¸åŒç±»åˆ«çš„è¯äº‘
        if labels is not None and len(labels) == len(text_data):
            unique_labels = pd.Series(labels).unique()
            for label in unique_labels[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªç±»åˆ«
                label_text = ' '.join(text_data[labels == label].dropna().astype(str))
                if label_text.strip():
                    wordcloud = WordCloud(
                        width=600,
                        height=300,
                        background_color='white',
                        max_words=50
                    ).generate(label_text)

                    fig = plt.figure(figsize=(8, 4))
                    plt.imshow(wordcloud, interpolation='bilinear')
                    plt.axis('off')
                    plt.title(f'{title} - ç±»åˆ« {label} è¯äº‘å›¾')

                    img_buf = io.BytesIO()
                    plt.savefig(img_buf, format='png', bbox_inches='tight')
                    img_buf.seek(0)

                    fig_plotly = px.imshow(
                        plt.imread(img_buf),
                        title=f'{title} - ç±»åˆ« {label} è¯äº‘å›¾'
                    )
                    visualizations[f'wordcloud_{label}'] = fig_plotly
                    plt.close()

    except Exception as e:
        st.warning(f"ç”Ÿæˆè¯äº‘å›¾æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    return visualizations


def get_model_files():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
    model_files = []
    models_dir = "../models"
    if os.path.exists(models_dir):
        for file in os.listdir(models_dir):
            if file.endswith('.pkl'):
                model_name = file.replace('.pkl', '')
                model_files.append(model_name)
    return sorted(model_files, reverse=True)  # æœ€æ–°çš„åœ¨å‰


def save_model_with_id(model, task_type, model_info=None):
    """ä¿å­˜æ¨¡å‹å¹¶æ·»åŠ æ—¥æœŸæ—¶é—´ID"""
    model_id = generate_model_id()
    model_name = f"{task_type}_model_{model_id}"
    models_dir = "../models"

    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    os.makedirs(models_dir, exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    if task_type == "classification":
        from pycaret.classification import save_model as save_clf_model
        save_clf_model(model, f"{models_dir}/{model_name}")
    elif task_type == "regression":
        from pycaret.regression import save_model as save_reg_model
        save_reg_model(model, f"{models_dir}/{model_name}")
    elif task_type == "clustering":
        from pycaret.clustering import save_model as save_cluster_model
        save_cluster_model(model, f"{models_dir}/{model_name}")

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    info_path = f"{models_dir}/{model_name}_info.txt"
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"æ¨¡å‹åç§°: {model_name}\n")
        f.write(f"ä»»åŠ¡ç±»å‹: {task_type}\n")
        f.write(f"åˆ›å»ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        if model_info:
            for key, value in model_info.items():
                f.write(f"{key}: {value}\n")

    return model_name


def create_clustering_visualizations(data, cluster_labels, n_clusters):
    """åˆ›å»ºèšç±»å¯è§†åŒ–å›¾è¡¨"""
    visualizations = {}

    # è·å–æ•°å€¼å‹åˆ—
    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()

    if len(numeric_columns) >= 2:
        # 1. æ•£ç‚¹å›¾ï¼ˆå‰ä¸¤ä¸ªä¸»è¦ç‰¹å¾ï¼‰
        fig1 = px.scatter(
            data,
            x=numeric_columns[0],
            y=numeric_columns[1],
            color=cluster_labels,
            title=f"èšç±»ç»“æœæ•£ç‚¹å›¾ ({numeric_columns[0]} vs {numeric_columns[1]})",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        visualizations['scatter'] = fig1

    if len(numeric_columns) >= 3:
        # 2. 3Dæ•£ç‚¹å›¾ï¼ˆå‰ä¸‰ä¸ªä¸»è¦ç‰¹å¾ï¼‰
        fig2 = px.scatter_3d(
            data,
            x=numeric_columns[0],
            y=numeric_columns[1],
            z=numeric_columns[2],
            color=cluster_labels,
            title=f"3Dèšç±»ç»“æœ ({numeric_columns[0]} vs {numeric_columns[1]} vs {numeric_columns[2]})",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        visualizations['scatter_3d'] = fig2

    # 3. èšç±»åˆ†å¸ƒé¥¼å›¾
    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
    fig3 = px.pie(
        values=cluster_counts.values,
        names=[f'èšç±» {i}' for i in cluster_counts.index],
        title='å„èšç±»æ ·æœ¬åˆ†å¸ƒ'
    )
    visualizations['pie'] = fig3

    # 4. èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿç‰¹å¾ï¼‰
    if len(numeric_columns) >= 2:
        data_with_clusters = data.copy()
        data_with_clusters['Cluster'] = cluster_labels

        # è®¡ç®—æ¯ä¸ªèšç±»çš„ä¸­å¿ƒç‚¹
        cluster_centers = data_with_clusters.groupby('Cluster')[numeric_columns].mean()

        fig4 = px.imshow(
            cluster_centers.T,
            labels=dict(x="èšç±»", y="ç‰¹å¾", color="å‡å€¼"),
            title="èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾",
            color_continuous_scale='RdYlBu_r'
        )
        visualizations['heatmap'] = fig4

    return visualizations


# æ”¯æŒæ–‡æœ¬çš„èšç±»ä»»åŠ¡å‡½æ•°
def clustering_task(data, n_clusters, features=None, include_text_features=False, text_columns=None):
    from pycaret.clustering import setup, create_model, assign_model, pull, plot_model
    from pycaret.clustering import save_model as save_cluster_model

    # åˆ†ç¦»æ•°å€¼å’Œæ–‡æœ¬ç‰¹å¾
    numeric_data = data.select_dtypes(include=[np.number])
    text_data = pd.DataFrame()

    # å¤„ç†æ–‡æœ¬ç‰¹å¾
    if include_text_features:
        text_columns = text_columns or []

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œè‡ªåŠ¨æ£€æµ‹
        if not text_columns:
            text_columns = data.select_dtypes(include=['object']).columns.tolist()
            text_columns = [col for col in text_columns if col not in features] if features else text_columns

        for col in text_columns:
            if col in data.columns:
                st.info(f"æ­£åœ¨å¤„ç†æ–‡æœ¬åˆ—: {col}")
                processed_text = preprocess_text_column(data[col])
                text_data[col] = processed_text

    # é€‰æ‹©ç”¨æˆ·æŒ‡å®šçš„ç‰¹å¾
    if features:
        available_numeric = [f for f in features if f in numeric_data.columns]
        available_text = [f for f in features if f in text_columns and f in text_data.columns]

        if available_numeric:
            numeric_data = numeric_data[available_numeric]
        if available_text:
            text_data = text_data[available_text]

    # å¦‚æœæ²¡æœ‰ç‰¹å¾ï¼Œè‡ªåŠ¨é€‰æ‹©
    if numeric_data.empty and text_data.empty:
        if not data.select_dtypes(include=[np.number]).empty:
            numeric_data = data.select_dtypes(include=[np.number])
        elif not data.select_dtypes(include=['object']).empty:
            auto_text_cols = data.select_dtypes(include=['object']).columns.tolist()[:2]  # æœ€å¤š2ä¸ªæ–‡æœ¬åˆ—
            for col in auto_text_cols:
                processed_text = preprocess_text_column(data[col])
                text_data[col] = processed_text

    if numeric_data.empty and text_data.empty:
        st.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨äºèšç±»åˆ†æçš„ç‰¹å¾")
        return None, None, None, None, None

    # åˆå¹¶æ•°å€¼å’Œæ–‡æœ¬ç‰¹å¾
    combined_data = numeric_data.copy()

    if not text_data.empty:
        # æå–æ–‡æœ¬ç‰¹å¾
        all_text_features = []
        feature_names = []

        for col in text_data.columns:
            if not text_data[col].empty and text_data[col].str.strip().any():
                features_matrix, names, vectorizer = extract_text_features(
                    text_data[col], max_features=100, method="tfidf"
                )
                if features_matrix is not None:
                    # è½¬æ¢ä¸ºDataFrame
                    text_features_df = pd.DataFrame(
                        features_matrix.toarray(),
                        columns=[f"{col}_{name}" for name in names]
                    )
                    all_text_features.append(text_features_df)
                    feature_names.extend([f"{col}_{name}" for name in names])

        if all_text_features:
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç‰¹å¾
            combined_text_features = pd.concat(all_text_features, axis=1)
            combined_data = pd.concat([combined_data, combined_text_features], axis=1)

            # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œä½¿ç”¨PCAé™ç»´
            if combined_data.shape[1] > 50:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=50, random_state=123)
                numeric_cols = combined_data.select_dtypes(include=[np.number]).columns
                combined_data[numeric_cols] = pca.fit_transform(combined_data[numeric_cols])
                st.info(f"ğŸ”§ ç‰¹å¾ç»´åº¦å·²é™ç»´è‡³50ç»´ä»¥ä¼˜åŒ–æ€§èƒ½")

    # è®¾ç½®èšç±»ç¯å¢ƒ
    with st.spinner("æ­£åœ¨è®¾ç½®èšç±»ç¯å¢ƒ..."):
        setup(data=combined_data, session_id=123, normalize=True, verbose=False)

    # åˆ›å»ºK-meansæ¨¡å‹
    with st.spinner("æ­£åœ¨è®­ç»ƒK-meansèšç±»æ¨¡å‹..."):
        kmeans_model = create_model('kmeans', num_clusters=n_clusters)

    st.success("âœ… èšç±»æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    # åˆ†é…èšç±»æ ‡ç­¾
    clustered_data = assign_model(kmeans_model)

    # åˆ›å»ºå¯è§†åŒ–
    visualizations = create_clustering_visualizations(numeric_data, clustered_data['Cluster'], n_clusters)

    # å¦‚æœæœ‰æ–‡æœ¬ç‰¹å¾ï¼Œæ·»åŠ æ–‡æœ¬å¯è§†åŒ–
    if not text_data.empty:
        text_visualizations = create_text_visualizations(
            text_data.iloc[:, 0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ—
            labels=clustered_data['Cluster'],
            title="æ–‡æœ¬èšç±»åˆ†æ"
        )
        visualizations.update(text_visualizations)

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    model_info = {
        "æ•°æ®é›†å¤§å°": f"{len(data)} è¡Œ",
        "åŸå§‹ç‰¹å¾æ•°é‡": f"{len(data.columns)} ä¸ª",
        "æ•°å€¼ç‰¹å¾æ•°é‡": f"{len(numeric_data.columns)} ä¸ª",
        "æ–‡æœ¬ç‰¹å¾æ•°é‡": f"{len(text_data.columns)} ä¸ª" if not text_data.empty else "0 ä¸ª",
        "èšç±»æ•°é‡": n_clusters,
        "èšç±»ç®—æ³•": "K-means",
        "ä½¿ç”¨çš„æ•°å€¼ç‰¹å¾": ", ".join(numeric_data.columns.tolist()) if not numeric_data.empty else "æ— ",
        "ä½¿ç”¨çš„æ–‡æœ¬ç‰¹å¾": ", ".join(text_data.columns.tolist()) if not text_data.empty else "æ— "
    }

    # è®¡ç®—èšç±»ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…æ•°å€¼ç‰¹å¾ï¼‰
    if not numeric_data.empty:
        cluster_stats = clustered_data.groupby('Cluster').agg({
            col: ['mean', 'std', 'count'] for col in numeric_data.columns
        }).round(3)
    else:
        cluster_stats = clustered_data.groupby('Cluster').size().reset_index(name='count')

    model_info["èšç±»ç»Ÿè®¡"] = f"å·²ç”Ÿæˆå„èšç±»çš„ç»Ÿè®¡ä¿¡æ¯"

    # ä½¿ç”¨æ–°çš„ä¿å­˜å‡½æ•°
    model_name = save_model_with_id(kmeans_model, "clustering", model_info)
    st.session_state.current_model_name = model_name

    # ä¿å­˜èšç±»ç»“æœï¼ˆåŒ…å«åŸå§‹æ•°æ®å’Œèšç±»æ ‡ç­¾ï¼‰
    result_data = data.copy()
    result_data['Cluster'] = clustered_data['Cluster']
    result_data.to_csv(f"../models/{model_name}_results.csv", index=False)

    return kmeans_model, clustered_data, model_name, visualizations, cluster_stats


# æ”¯æŒæ–‡æœ¬çš„åˆ†ç±»ä»»åŠ¡å‡½æ•°
def classification_task(data, target_variable, train_size, preprocess_text=False, text_columns=None):
    from pycaret.classification import setup, compare_models, save_model, pull, plot_model, predict_model
    from pycaret.classification import save_model as save_clf_model

    # å¤„ç†æ–‡æœ¬é¢„å¤„ç†
    processed_data = data.copy()
    text_processing_info = {"æ–‡æœ¬åˆ—æ•°é‡": 0, "å¤„ç†çš„æ–‡æœ¬åˆ—": []}

    if preprocess_text:
        text_columns = text_columns or []

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œè‡ªåŠ¨æ£€æµ‹
        if not text_columns:
            text_columns = data.select_dtypes(include=['object']).columns.tolist()
            text_columns = [col for col in text_columns if col != target_variable]

        for col in text_columns:
            if col in data.columns and col != target_variable:
                st.info(f"æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬åˆ—: {col}")
                processed_data[col] = preprocess_text_column(data[col])
                text_processing_info["å¤„ç†çš„æ–‡æœ¬åˆ—"].append(col)
                text_processing_info["æ–‡æœ¬åˆ—æ•°é‡"] += 1

    # è®¾ç½®åˆ†ç±»ç¯å¢ƒ
    setup(data=processed_data, target=target_variable, session_id=123, normalize=True,
          train_size=train_size)

    with st.spinner("æ­£åœ¨è®­ç»ƒå’Œæ¯”è¾ƒåˆ†ç±»æ¨¡å‹..."):
        best_model = compare_models()
    st.success("âœ… åˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    model_comparison = pull()
    best_model_name = str(best_model)
    accuracy = model_comparison.loc['Accuracy', best_model_name] if 'Accuracy' in model_comparison.index else 'N/A'

    # ç»Ÿè®¡ç‰¹å¾ç±»å‹
    numeric_features = len(processed_data.select_dtypes(include=[np.number]).columns) - 1  # å‡å»ç›®æ ‡å˜é‡
    text_features = len([col for col in processed_data.columns if processed_data[col].dtype == 'object' and col != target_variable])

    model_info = {
        "æ•°æ®é›†å¤§å°": f"{len(data)} è¡Œ",
        "æ•°å€¼ç‰¹å¾æ•°é‡": f"{numeric_features} ä¸ª",
        "æ–‡æœ¬ç‰¹å¾æ•°é‡": f"{text_features} ä¸ª",
        "ç›®æ ‡å˜é‡": target_variable,
        "è®­ç»ƒé›†æ¯”ä¾‹": f"{train_size:.1%}",
        "æœ€ä½³æ¨¡å‹": best_model_name,
        "å‡†ç¡®ç‡": f"{accuracy:.4f}" if accuracy != 'N/A' else 'N/A',
        **text_processing_info
    }

    # ä½¿ç”¨æ–°çš„ä¿å­˜å‡½æ•°
    model_name = save_model_with_id(best_model, "classification", model_info)
    st.session_state.current_model_name = model_name

    # ç”Ÿæˆæ–‡æœ¬å¯è§†åŒ–ï¼ˆå¦‚æœæœ‰æ–‡æœ¬ç‰¹å¾ï¼‰
    text_visualizations = {}
    if preprocess_text and text_processing_info["æ–‡æœ¬åˆ—æ•°é‡"] > 0:
        # ä¸ºç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ—åˆ›å»ºè¯äº‘å›¾
        first_text_col = text_processing_info["å¤„ç†çš„æ–‡æœ¬åˆ—"][0]
        text_visualizations = create_text_visualizations(
            processed_data[first_text_col],
            labels=data[target_variable],
            title=f"åˆ†ç±»ä»»åŠ¡ - {first_text_col}"
        )

    return best_model, model_comparison, model_name, text_visualizations


# æ”¯æŒæ–‡æœ¬çš„å›å½’ä»»åŠ¡å‡½æ•°
def regression_task(data, target_variable, train_size, preprocess_text=False, text_columns=None):
    from pycaret.regression import setup, compare_models, save_model, pull, predict_model
    from pycaret.regression import save_model as save_reg_model

    # å¤„ç†æ–‡æœ¬é¢„å¤„ç†
    processed_data = data.copy()
    text_processing_info = {"æ–‡æœ¬åˆ—æ•°é‡": 0, "å¤„ç†çš„æ–‡æœ¬åˆ—": []}

    if preprocess_text:
        text_columns = text_columns or []

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œè‡ªåŠ¨æ£€æµ‹
        if not text_columns:
            text_columns = data.select_dtypes(include=['object']).columns.tolist()
            text_columns = [col for col in text_columns if col != target_variable]

        for col in text_columns:
            if col in data.columns and col != target_variable:
                st.info(f"æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬åˆ—: {col}")
                processed_data[col] = preprocess_text_column(data[col])
                text_processing_info["å¤„ç†çš„æ–‡æœ¬åˆ—"].append(col)
                text_processing_info["æ–‡æœ¬åˆ—æ•°é‡"] += 1

    # è®¾ç½®å›å½’ç¯å¢ƒ
    setup(data=processed_data, target=target_variable, train_size=train_size)

    with st.spinner("æ­£åœ¨è®­ç»ƒå’Œæ¯”è¾ƒå›å½’æ¨¡å‹..."):
        best_model = compare_models()
    st.success("âœ… å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    model_comparison = pull()
    best_model_name = str(best_model)
    r2 = model_comparison.loc['R2', best_model_name] if 'R2' in model_comparison.index else 'N/A'
    rmse = model_comparison.loc['RMSE', best_model_name] if 'RMSE' in model_comparison.index else 'N/A'

    # ç»Ÿè®¡ç‰¹å¾ç±»å‹
    numeric_features = len(processed_data.select_dtypes(include=[np.number]).columns) - 1  # å‡å»ç›®æ ‡å˜é‡
    text_features = len([col for col in processed_data.columns if processed_data[col].dtype == 'object' and col != target_variable])

    model_info = {
        "æ•°æ®é›†å¤§å°": f"{len(data)} è¡Œ",
        "æ•°å€¼ç‰¹å¾æ•°é‡": f"{numeric_features} ä¸ª",
        "æ–‡æœ¬ç‰¹å¾æ•°é‡": f"{text_features} ä¸ª",
        "ç›®æ ‡å˜é‡": target_variable,
        "è®­ç»ƒé›†æ¯”ä¾‹": f"{train_size:.1%}",
        "æœ€ä½³æ¨¡å‹": best_model_name,
        "RÂ² åˆ†æ•°": f"{r2:.4f}" if r2 != 'N/A' else 'N/A',
        "RMSE": f"{rmse:.4f}" if rmse != 'N/A' else 'N/A',
        **text_processing_info
    }

    # ä½¿ç”¨æ–°çš„ä¿å­˜å‡½æ•°
    model_name = save_model_with_id(best_model, "regression", model_info)
    st.session_state.current_model_name = model_name

    # ç”Ÿæˆæ–‡æœ¬å¯è§†åŒ–ï¼ˆå¦‚æœæœ‰æ–‡æœ¬ç‰¹å¾ï¼‰
    text_visualizations = {}
    if preprocess_text and text_processing_info["æ–‡æœ¬åˆ—æ•°é‡"] > 0:
        # ä¸ºç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ—åˆ›å»ºè¯äº‘å›¾
        first_text_col = text_processing_info["å¤„ç†çš„æ–‡æœ¬åˆ—"][0]
        text_visualizations = create_text_visualizations(
            processed_data[first_text_col],
            labels=data[target_variable],
            title=f"å›å½’ä»»åŠ¡ - {first_text_col}"
        )

    return best_model, model_comparison, model_name, text_visualizations


# é¢„æµ‹å‡½æ•°
def prediction(model_path, prediction_file):
    try:
        models_dir = "../models"
        full_model_path = f"{models_dir}/{model_path}"

        if os.path.exists(f'{full_model_path}.pkl'):
            if 'classification' in model_path:
                from pycaret.classification import load_model, predict_model
            elif 'regression' in model_path:
                from pycaret.regression import load_model, predict_model
            elif 'clustering' in model_path:
                from pycaret.clustering import load_model, assign_model
                # èšç±»ä»»åŠ¡çš„ç‰¹æ®Šå¤„ç†
                loaded_model = load_model(full_model_path)
                st.success("âœ… èšç±»æ¨¡å‹å·²æˆåŠŸè½½å…¥")

                # è¯»å–å¾…é¢„æµ‹æ•°æ®
                if prediction_file.name.endswith('.csv'):
                    prediction_data = pd.read_csv(prediction_file, encoding='utf-8-sig')
                elif prediction_file.name.endswith('.xlsx'):
                    prediction_data = pd.read_excel(prediction_file, engine='openpyxl')

                # åªä¿ç•™æ•°å€¼å‹ç‰¹å¾
                numeric_prediction_data = prediction_data.select_dtypes(include=[np.number])

                # è¿›è¡Œèšç±»é¢„æµ‹
                clustered_prediction = assign_model(loaded_model, data=numeric_prediction_data)
                st.success("âœ… èšç±»é¢„æµ‹å®Œæˆï¼")
                st.write("èšç±»ç»“æœï¼š")
                st.dataframe(clustered_prediction)

                # æä¾›ä¸‹è½½åŠŸèƒ½
                csv = clustered_prediction.to_csv(index=False)
                st.download_button(
                    label="ä¸‹è½½èšç±»ç»“æœ (CSV)",
                    data=csv,
                    file_name=f"clustering_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
                return

            # åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„é€šç”¨å¤„ç†
            loaded_model = load_model(full_model_path)
            st.success("âœ… æ¨¡å‹å·²æˆåŠŸè½½å…¥")

            # è¯»å–å¾…é¢„æµ‹æ•°æ®
            if prediction_file.name.endswith('.csv'):
                prediction_data = pd.read_csv(prediction_file, encoding='utf-8-sig')
            elif prediction_file.name.endswith('.xlsx'):
                prediction_data = pd.read_excel(prediction_file, engine='openpyxl')

            predictions = predict_model(loaded_model, data=prediction_data)
            st.success("âœ… é¢„æµ‹å®Œæˆï¼")
            st.write("é¢„æµ‹ç»“æœï¼š")
            st.dataframe(predictions)

            # æä¾›ä¸‹è½½åŠŸèƒ½
            csv = predictions.to_csv(index=False)
            st.download_button(
                label="ä¸‹è½½é¢„æµ‹ç»“æœ (CSV)",
                data=csv,
                file_name=f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

        else:
            st.error("âŒ æœªæ‰¾åˆ°ç›¸åº”çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹")
    except Exception as e:
        st.error(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")


def export_model(model_name):
    """å¯¼å‡ºæ¨¡å‹æ–‡ä»¶"""
    try:
        models_dir = "../models"
        model_path = f"{models_dir}/{model_name}"

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # å¤åˆ¶æ¨¡å‹æ–‡ä»¶
            shutil.copy(f"{model_path}.pkl", temp_dir)

            # å¤åˆ¶æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            info_file = f"{model_path}_info.txt"
            if os.path.exists(info_file):
                shutil.copy(info_file, temp_dir)

            # å¤åˆ¶èšç±»ç»“æœæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            results_file = f"{model_path}_results.csv"
            if os.path.exists(results_file):
                shutil.copy(results_file, temp_dir)

            # åˆ›å»ºzipæ–‡ä»¶
            zip_path = f"{temp_dir}/{model_name}.zip"
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                zipf.write(f"{temp_dir}/{model_name}.pkl", f"{model_name}.pkl")
                if os.path.exists(info_file):
                    zipf.write(f"{temp_dir}/{model_name}_info.txt", f"{model_name}_info.txt")
                if os.path.exists(results_file):
                    zipf.write(f"{temp_dir}/{model_name}_results.csv", f"{model_name}_results.csv")

            # è¯»å–zipæ–‡ä»¶å¹¶è¿”å›
            with open(zip_path, 'rb') as f:
                zip_data = f.read()

            return zip_data
    except Exception as e:
        st.error(f"å¯¼å‡ºæ¨¡å‹æ—¶å‡ºç°é”™è¯¯: {str(e)}")
        return None


def import_model(uploaded_file):
    """å¯¼å…¥æ¨¡å‹æ–‡ä»¶"""
    try:
        models_dir = "../models"
        os.makedirs(models_dir, exist_ok=True)

        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_path = f"temp_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        # è§£å‹æ–‡ä»¶
        with zipfile.ZipFile(temp_path, 'r') as zipf:
            zipf.extractall(models_dir)

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)

        st.success("âœ… æ¨¡å‹å¯¼å…¥æˆåŠŸï¼")
        return True
    except Exception as e:
        st.error(f"å¯¼å…¥æ¨¡å‹æ—¶å‡ºç°é”™è¯¯: {str(e)}")
        return False


def show_model_info(model_name):
    """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
    try:
        models_dir = "../models"
        info_file = f"{models_dir}/{model_name}_info.txt"

        if os.path.exists(info_file):
            with open(info_file, 'r', encoding='utf-8') as f:
                info = f.read()
            st.info(f"ğŸ“‹ **æ¨¡å‹ä¿¡æ¯**\n\n```\n{info}\n```")
        else:
            st.info("ğŸ“‹ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")
    except Exception as e:
        st.warning(f"è¯»å–æ¨¡å‹ä¿¡æ¯æ—¶å‡ºç°é”™è¯¯: {str(e)}")


# å®šä¹‰ä¸»å‡½æ•°
def main():
    st.title("MLquick - æœºå™¨å­¦ä¹ é›¶ä»£ç åº”ç”¨å¹³å°")

    # ä¾§è¾¹æ  - æ¨¡å‹ç®¡ç†
    st.sidebar.markdown("## ğŸ”§ æ¨¡å‹ç®¡ç†")

    # æ˜¾ç¤ºå½“å‰æ¨¡å‹
    if 'current_model_name' in st.session_state and st.session_state.current_model_name:
        st.sidebar.success(f"å½“å‰æ¨¡å‹: {st.session_state.current_model_name}")
        show_model_info(st.session_state.current_model_name)

    # æ¨¡å‹å¯¼å‡º
    if 'current_model_name' in st.session_state and st.session_state.current_model_name:
        if st.sidebar.button("ğŸ“¤ å¯¼å‡ºå½“å‰æ¨¡å‹"):
            zip_data = export_model(st.session_state.current_model_name)
            if zip_data:
                st.sidebar.download_button(
                    label=f"ä¸‹è½½ {st.session_state.current_model_name}",
                    data=zip_data,
                    file_name=f"{st.session_state.current_model_name}.zip",
                    mime="application/zip"
                )

    # æ¨¡å‹å¯¼å…¥
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“¥ å¯¼å…¥æ¨¡å‹")
    uploaded_model = st.sidebar.file_uploader("ä¸Šä¼ æ¨¡å‹æ–‡ä»¶ (.zip)", type=["zip"])
    if uploaded_model is not None:
        if st.sidebar.button("å¯¼å…¥æ¨¡å‹"):
            if import_model(uploaded_model):
                st.rerun()

    # å¯ç”¨æ¨¡å‹åˆ—è¡¨
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“‚ å¯ç”¨æ¨¡å‹")
    model_files = get_model_files()
    if model_files:
        selected_model = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", model_files)
        if st.sidebar.button("åŠ è½½é€‰ä¸­æ¨¡å‹"):
            st.session_state.current_model_name = selected_model
            st.rerun()
    else:
        st.sidebar.info("æš‚æ— å¯ç”¨æ¨¡å‹")

    # ä¸»ç•Œé¢
    st.markdown("---")

    # ä¸Šä¼ æ•°æ®
    uploaded_file = st.file_uploader("ğŸ“ ä¸Šä¼ æ•°æ®é›† (CSV æˆ– Excelæ ¼å¼)", type=["csv", "xlsx"])

    if uploaded_file is not None:
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è¯»å–æ•°æ®
        if uploaded_file.name.endswith('.csv'):
            data = pd.read_csv(uploaded_file, encoding='utf-8-sig')
        elif uploaded_file.name.endswith('.xlsx'):
            data = pd.read_excel(uploaded_file, engine='openpyxl')

        data = pd.DataFrame(data)
        st.markdown("### ğŸ“Š æ•°æ®é¢„è§ˆ")
        st.write(f"æ•°æ®å½¢çŠ¶: {data.shape[0]} è¡Œ Ã— {data.shape[1]} åˆ—")
        st.dataframe(data.head(10))

        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        text_columns = data.select_dtypes(include=['object']).columns.tolist()
        st.info(f"ğŸ“ˆ **æ•°æ®ç»Ÿè®¡**: æ•°å€¼å‹ç‰¹å¾ {len(numeric_columns)} ä¸ªï¼Œæ–‡æœ¬ç‰¹å¾ {len(text_columns)} ä¸ªï¼Œæ€»ç‰¹å¾ {len(data.columns)} ä¸ª")

        # é€‰æ‹©ä»»åŠ¡ç±»å‹
        st.markdown("### âš™ï¸ æ¨¡å‹é…ç½®")
        task_type = st.selectbox("é€‰æ‹©ä»»åŠ¡ç±»å‹", ["åˆ†ç±»", "å›å½’", "èšç±»"])

        # æ–‡æœ¬å¤„ç†é€‰é¡¹
        text_processing_available = len(text_columns) > 0
        preprocess_text = False
        selected_text_columns = []

        if text_processing_available:
            st.markdown("#### ğŸ“ æ–‡æœ¬å¤„ç†é€‰é¡¹")
            preprocess_text = st.checkbox("å¯ç”¨æ–‡æœ¬é¢„å¤„ç†", value=False,
                                        help="å¯¹æ–‡æœ¬ç‰¹å¾è¿›è¡Œåˆ†è¯ã€åœç”¨è¯ç§»é™¤ç­‰é¢„å¤„ç†")

            if preprocess_text:
                text_processing_method = st.radio("æ–‡æœ¬å¤„ç†æ–¹å¼", ["è‡ªåŠ¨æ£€æµ‹", "æ‰‹åŠ¨é€‰æ‹©"],
                                                help="è‡ªåŠ¨æ£€æµ‹è¯­è¨€ç±»å‹æˆ–æ‰‹åŠ¨é€‰æ‹©éœ€è¦å¤„ç†çš„æ–‡æœ¬åˆ—")

                if text_processing_method == "æ‰‹åŠ¨é€‰æ‹©":
                    selected_text_columns = st.multiselect(
                        "é€‰æ‹©è¦å¤„ç†çš„æ–‡æœ¬åˆ—",
                        text_columns,
                        default=text_columns,
                        help="é€‰æ‹©éœ€è¦è¿›è¡Œé¢„å¤„ç†çš„æ–‡æœ¬åˆ—"
                    )
                else:
                    selected_text_columns = text_columns

                # æ–‡æœ¬é¢„å¤„ç†å‚æ•°
                col1, col2 = st.columns(2)
                with col1:
                    remove_stopwords = st.checkbox("ç§»é™¤åœç”¨è¯", value=True,
                                                help="ç§»é™¤å¸¸è§ä½†æ— æ„ä¹‰çš„è¯è¯­")
                with col2:
                    min_word_length = st.number_input("æœ€å°è¯é•¿åº¦", min_value=1, max_value=5,
                                                    value=2, help="è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯è¯­")
        else:
            st.info("ğŸ“ æ•°æ®ä¸­æœªæ£€æµ‹åˆ°æ–‡æœ¬ç‰¹å¾ï¼Œæ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½ä¸å¯ç”¨")

        if task_type == "èšç±»":
            # èšç±»ä»»åŠ¡ç‰¹æ®Šé…ç½®
            st.markdown("#### ğŸ¯ èšç±»é…ç½®")

            # èšç±»æ•°é‡
            n_clusters = st.number_input(
                "èšç±»æ•°é‡ (Kå€¼)",
                min_value=2,
                max_value=min(20, len(data)),
                value=3,
                step=1,
                help="K-meansèšç±»çš„ç±»åˆ«æ•°é‡"
            )

            # æ–‡æœ¬èšç±»é€‰é¡¹
            include_text_features = False
            if text_processing_available:
                include_text_features = st.checkbox(
                    "åŒ…å«æ–‡æœ¬ç‰¹å¾è¿›è¡Œèšç±»",
                    value=False,
                    help="å°†æ–‡æœ¬ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾åç”¨äºèšç±»åˆ†æ"
                )

                if include_text_features:
                    clustering_text_method = st.radio(
                        "èšç±»æ–‡æœ¬é€‰æ‹©æ–¹å¼",
                        ["ä½¿ç”¨æ‰€æœ‰æ–‡æœ¬ç‰¹å¾", "æ‰‹åŠ¨é€‰æ‹©"],
                        help="é€‰æ‹©ç”¨äºèšç±»çš„æ–‡æœ¬ç‰¹å¾"
                    )

                    if clustering_text_method == "æ‰‹åŠ¨é€‰æ‹©":
                        clustering_text_columns = st.multiselect(
                            "é€‰æ‹©ç”¨äºèšç±»çš„æ–‡æœ¬ç‰¹å¾",
                            text_columns,
                            default=text_columns[:1] if text_columns else [],
                            help="é€‰æ‹©ç”¨äºèšç±»åˆ†æçš„æ–‡æœ¬ç‰¹å¾"
                        )
                    else:
                        clustering_text_columns = text_columns

            # ç‰¹å¾é€‰æ‹©ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
            available_features = numeric_columns
            if include_text_features and text_columns:
                available_features = numeric_columns + text_columns

            if available_features:
                selected_features = st.multiselect(
                    "é€‰æ‹©ç”¨äºèšç±»çš„ç‰¹å¾ (ç•™ç©ºåˆ™è‡ªåŠ¨é€‰æ‹©)",
                    available_features,
                    default=available_features[:min(5, len(available_features))],  # é»˜è®¤é€‰æ‹©å‰5ä¸ªç‰¹å¾
                    help="é€‰æ‹©ç”¨äºèšç±»åˆ†æçš„ç‰¹å¾ï¼Œæ”¯æŒæ•°å€¼å’Œæ–‡æœ¬ç‰¹å¾"
                )
            else:
                st.warning("âš ï¸ æ•°æ®ä¸­æ²¡æœ‰å¯ç”¨äºèšç±»åˆ†æçš„ç‰¹å¾")
                selected_features = []

        else:
            # åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„é…ç½®
            target_variable = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡", data.columns)
            train_size = st.number_input("è¾“å…¥è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0 - 1ä¹‹é—´ï¼‰", min_value=0.0, max_value=1.0, value=0.7, step=0.01)

        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        if 'best_model' not in st.session_state:
            st.session_state.best_model = None
        if 'model_comparison' not in st.session_state:
            st.session_state.model_comparison = None
        if 'clustered_data' not in st.session_state:
            st.session_state.clustered_data = None
        if 'visualizations' not in st.session_state:
            st.session_state.visualizations = None
        if 'text_visualizations' not in st.session_state:
            st.session_state.text_visualizations = None
        if 'cluster_stats' not in st.session_state:
            st.session_state.cluster_stats = None

        # è®­ç»ƒæ¨¡å‹
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹", type="primary"):
            with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                if task_type == "èšç±»":
                    # è·å–æ–‡æœ¬èšç±»å‚æ•°
                    clustering_text_cols = []
                    if text_processing_available and include_text_features:
                        clustering_text_cols = clustering_text_columns if 'clustering_text_columns' in locals() else text_columns

                    model, clustered_data, model_name, visualizations, cluster_stats = clustering_task(
                        data, n_clusters, selected_features, include_text_features, clustering_text_cols)
                    if model is not None:
                        st.session_state.best_model = model
                        st.session_state.clustered_data = clustered_data
                        st.session_state.visualizations = visualizations
                        st.session_state.cluster_stats = cluster_stats

                elif task_type == "åˆ†ç±»":
                    best_model, model_comparison, model_name, text_visualizations = classification_task(
                        data, target_variable, train_size, preprocess_text, selected_text_columns)
                    st.session_state.best_model = best_model
                    st.session_state.model_comparison = model_comparison
                    st.session_state.text_visualizations = text_visualizations
                else:  # å›å½’
                    best_model, model_comparison, model_name, text_visualizations = regression_task(
                        data, target_variable, train_size, preprocess_text, selected_text_columns)
                    st.session_state.best_model = best_model
                    st.session_state.model_comparison = model_comparison
                    st.session_state.text_visualizations = text_visualizations

        # æ˜¾ç¤ºç»“æœ
        if task_type == "èšç±»" and st.session_state.clustered_data is not None:
            st.markdown("### ğŸ“ˆ èšç±»åˆ†æç»“æœ")

            # æ˜¾ç¤ºèšç±»ç»Ÿè®¡ä¿¡æ¯
            if 'cluster_stats' in st.session_state:
                st.markdown("#### ğŸ“Š èšç±»ç»Ÿè®¡ä¿¡æ¯")
                st.dataframe(st.session_state.cluster_stats)

            # æ˜¾ç¤ºå¯è§†åŒ–
            if st.session_state.visualizations:
                st.markdown("#### ğŸ¨ èšç±»å¯è§†åŒ–")

                # æ•£ç‚¹å›¾
                if 'scatter' in st.session_state.visualizations:
                    st.plotly_chart(st.session_state.visualizations['scatter'], use_container_width=True)

                # 3Dæ•£ç‚¹å›¾
                if 'scatter_3d' in st.session_state.visualizations:
                    st.plotly_chart(st.session_state.visualizations['scatter_3d'], use_container_width=True)

                # é¥¼å›¾
                if 'pie' in st.session_state.visualizations:
                    col1, col2 = st.columns(2)
                    with col1:
                        st.plotly_chart(st.session_state.visualizations['pie'], use_container_width=True)

                # çƒ­åŠ›å›¾
                if 'heatmap' in st.session_state.visualizations:
                    with col2:
                        st.plotly_chart(st.session_state.visualizations['heatmap'], use_container_width=True)

            # ä¸‹è½½èšç±»ç»“æœ
            csv = st.session_state.clustered_data.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½èšç±»ç»“æœ (CSV)",
                data=csv,
                file_name=f"clustering_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

        elif task_type != "èšç±»" and st.session_state.model_comparison is not None:
            st.markdown("### ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
            st.dataframe(st.session_state.model_comparison)

            # æ˜¾ç¤ºæ–‡æœ¬å¯è§†åŒ–
            if st.session_state.text_visualizations and len(st.session_state.text_visualizations) > 0:
                st.markdown("### ğŸ“ æ–‡æœ¬åˆ†æå¯è§†åŒ–")
                viz_count = 0
                for viz_name, viz_chart in st.session_state.text_visualizations.items():
                    if viz_count < 6:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                        st.plotly_chart(viz_chart, use_container_width=True)
                        viz_count += 1
                    else:
                        break

        # é¢„æµ‹åŠŸèƒ½
        st.markdown("---")
        st.markdown("### ğŸ”® æ¨¡å‹é¢„æµ‹")

        # é€‰æ‹©é¢„æµ‹æ–¹å¼
        prediction_mode = st.radio("é€‰æ‹©é¢„æµ‹æ–¹å¼", ["ä½¿ç”¨å½“å‰è®­ç»ƒçš„æ¨¡å‹", "é€‰æ‹©å·²æœ‰æ¨¡å‹"])

        if prediction_mode == "ä½¿ç”¨å½“å‰è®­ç»ƒçš„æ¨¡å‹":
            if 'current_model_name' in st.session_state and st.session_state.current_model_name:
                st.info(f"ä½¿ç”¨æ¨¡å‹: {st.session_state.current_model_name}")
                prediction_file = st.file_uploader("ğŸ“ ä¸Šä¼ å¾…é¢„æµ‹æ•°æ®", type=["csv", "xlsx"], key="pred_current")
                if prediction_file is not None:
                    prediction(st.session_state.current_model_name, prediction_file)
            else:
                st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹")

        else:  # é€‰æ‹©å·²æœ‰æ¨¡å‹
            model_files = get_model_files()
            if model_files:
                selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_files, key="pred_model_select")
                st.info(f"ä½¿ç”¨æ¨¡å‹: {selected_model}")
                prediction_file = st.file_uploader("ğŸ“ ä¸Šä¼ å¾…é¢„æµ‹æ•°æ®", type=["csv", "xlsx"], key="pred_existing")
                if prediction_file is not None:
                    prediction(selected_model, prediction_file)
            else:
                st.warning("æš‚æ— å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–å¯¼å…¥æ¨¡å‹")

    # é¡µè„š
    st.markdown("---")
    st.markdown("### ğŸ’¡ ä½¿ç”¨æç¤º")
    st.markdown("""
    - **åˆ†ç±»ä»»åŠ¡**: éœ€è¦é€‰æ‹©ç›®æ ‡å˜é‡ï¼Œç”¨äºé¢„æµ‹ç±»åˆ«
    - **å›å½’ä»»åŠ¡**: éœ€è¦é€‰æ‹©ç›®æ ‡å˜é‡ï¼Œç”¨äºé¢„æµ‹æ•°å€¼
    - **èšç±»ä»»åŠ¡**: è‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„ç¾¤ç»„ï¼Œæ— éœ€ç›®æ ‡å˜é‡
    - **æ–‡æœ¬å¤„ç†**: æ”¯æŒä¸­è‹±æ–‡æ–‡æœ¬é¢„å¤„ç†ï¼ŒåŒ…æ‹¬åˆ†è¯ã€åœç”¨è¯ç§»é™¤ç­‰
    - **æ–‡æœ¬èšç±»**: å¯å°†æ–‡æœ¬ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾è¿›è¡Œèšç±»åˆ†æ
    - **æ–‡æœ¬å¯è§†åŒ–**: è‡ªåŠ¨ç”Ÿæˆè¯äº‘å›¾ç­‰æ–‡æœ¬åˆ†æå¯è§†åŒ–
    - æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: CSV (.csv), Excel (.xlsx)
    - æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜ï¼Œä¸ä¼šè¦†ç›–ä¹‹å‰çš„æ¨¡å‹
    - å¯ä»¥é€šè¿‡ä¾§è¾¹æ å¯¼å…¥/å¯¼å‡ºæ¨¡å‹
    - é¢„æµ‹ç»“æœå¯ä»¥ä¸‹è½½ä¸ºCSVæ–‡ä»¶
    - å»ºè®®è®­ç»ƒé›†æ¯”ä¾‹è®¾ç½®åœ¨0.6-0.8ä¹‹é—´ï¼ˆä»…å¯¹åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼‰
    - æ–‡æœ¬æ ·ä¾‹æ•°æ®ä½äº `data/samples/text_*_sample.csv`
    """)


if __name__ == "__main__":
    main()